{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7da397-dc3a-4c5b-ba92-4042c7809247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting arxiv\n",
      "  Downloading arxiv-2.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser==6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests==2.31.0 (from arxiv)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests==2.31.0->arxiv) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests==2.31.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests==2.31.0->arxiv) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests==2.31.0->arxiv) (2022.12.7)\n",
      "Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m288.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m518.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=a118322395a53064762368aead65d447a5f149c1af548e13171a34d513dc9167\n",
      "  Stored in directory: /Users/qiupengchao/Library/Caches/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built sgmllib3k\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sgmllib3k, requests, feedparser, arxiv\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Not uninstalling requests at /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages, outside environment /Users/qiupengchao/lab/venv\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed arxiv-2.1.0 feedparser-6.0.10 requests-2.31.0 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5091af-a2fd-459b-8462-f23cea9db6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1f93d7-90c0-4d5f-9024-d97bc0704688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6dce8e6-af26-4994-a311-3ffb292cbdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for the paper with the identifier \"2005.14165\" on arxiv to find out its innovative points.\n",
      "Action: arxiv\n",
      "Action Input: 2005.14165\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2020-07-22\n",
      "Title: Language Models are Few-Shot Learners\n",
      "Authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\n",
      "Summary: Recent work has demonstrated substantial gains on many NLP tasks and\n",
      "benchmarks by pre-training on a large corpus of text followed by fine-tuning on\n",
      "a specific task. While typically task-agnostic in architecture, this method\n",
      "still requires task-specific fine-tuning datasets of thousands or tens of\n",
      "thousands of examples. By contrast, humans can generally perform a new language\n",
      "task from only a few examples or from simple instructions - something which\n",
      "current NLP systems still largely struggle to do. Here we show that scaling up\n",
      "language models greatly improves task-agnostic, few-shot performance, sometimes\n",
      "even reaching competitiveness with prior state-of-the-art fine-tuning\n",
      "approaches. Specifically, we train GPT-3, an autoregressive language model with\n",
      "175 billion parameters, 10x more than any previous non-sparse language model,\n",
      "and test its performance in the few-shot setting. For all tasks, GPT-3 is\n",
      "applied without any gradient updates or fine-tuning, with tasks and few-shot\n",
      "demonstrations specified purely via text interaction with the model. GPT-3\n",
      "achieves strong performance on many NLP datasets, including translation,\n",
      "question-answering, and cloze tasks, as well as several tasks that require\n",
      "on-the-fly reasoning or domain adaptation, such as unscrambling words, using a\n",
      "novel word in a sentence, or performing 3-digit arithmetic. At the same time,\n",
      "we also identify some datasets where GPT-3's few-shot learning still struggles,\n",
      "as well as some datasets where GPT-3 faces methodological issues related to\n",
      "training on large web corpora. Finally, we find that GPT-3 can generate samples\n",
      "of news articles which human evaluators have difficulty distinguishing from\n",
      "articles written by humans. We discuss broader societal impacts of this finding\n",
      "and of GPT-3 in general.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe innovative point of this paper is that it demonstrates how scaling up language models, specifically GPT-3 with 175 billion parameters, greatly improves task-agnostic, few-shot performance in natural language processing tasks. This means that the model can perform well on various tasks with only a few examples or simple instructions, without the need for extensive fine-tuning or gradient updates. \n",
      "\n",
      "Final Answer: The innovative point of paper 2005.14165 is the significant improvement in few-shot performance of language models, particularly GPT-3, without the need for task-specific fine-tuning datasets.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '介绍下2005.14165 这篇论文的创新点？',\n",
       " 'output': 'The innovative point of paper 2005.14165 is the significant improvement in few-shot performance of language models, particularly GPT-3, without the need for task-specific fine-tuning datasets.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"]\n",
    ")\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent= AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent_chain.invoke(\"介绍下2005.14165 这篇论文的创新点？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
